services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:latest
    container_name: kafka-broker
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-broker:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    command:
      - sh
      - -c
      - |
        /etc/confluent/docker/run &
        while ! nc -z kafka-broker 9092; do
          sleep 1;
        done;
        kafka-topics --create --topic btcirt_topic  --replication-factor 1 --if-not-exists --bootstrap-server kafka-broker:9092;
        kafka-topics --create --topic usdtirt_topic --replication-factor 1 --if-not-exists --bootstrap-server kafka-broker:9092;
        kafka-topics --create --topic ethirt_topic --replication-factor 1 --if-not-exists --bootstrap-server kafka-broker:9092;
        kafka-topics --create --topic etcirt_topic --replication-factor 1 --if-not-exists --bootstrap-server kafka-broker:9092;
        kafka-topics --create --topic shibirt_topic --replication-factor 1 --if-not-exists --bootstrap-server kafka-broker:9092;
        wait

  schema-registry:
    image: confluentinc/cp-schema-registry:latest
    container_name: schema-registry
    depends_on:
      - kafka
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: "zookeeper:2181"
      SCHEMA_REGISTRY_LISTENERS: "http://schema-registry:8081"
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: "PLAINTEXT://kafka-broker:9092"

  data-ingestion:
    build:
      context: ./services/data-ingestion
      dockerfile: Dockerfile
    container_name: data-ingestion
    depends_on:
      - kafka
    environment:
      FLASK_HOST: "0.0.0.0"
      FLASK_PORT: 5000
      KAFKA_BROKER: "kafka-broker:9092"
      KAFKA_TOPIC: "stock_data"
    ports:
      - "5000:5000"

  stream-processing:
    build:
      context: ./services/stream-processing
      dockerfile: Dockerfile
    container_name: stream-processing
    depends_on:
      - kafka
    environment:
      KAFKA_BROKER: "kafka-broker:9092"
    command: python /app/consumer.py

  questdb:
    image: questdb/questdb:latest
    container_name: questdb
    # QuestDB default ports:
    # - 9000 (HTTP Web Console)
    # - 8812 (Postgres wire)
    # - 9009 (ILP)
    ports:
      - "9000:9000"
      - "8812:8812"
    volumes:
      - questdb-volume:/root/.questdb

  # aggregator:
  #   build:
  #     context: ./aggregator
  #     dockerfile: Dockerfile
  #   container_name: aggregator
  #   depends_on:
  #     - kafka
  #     - questdb
  #   environment:
  #     # Kafka connection settings
  #     KAFKA_BOOTSTRAP_SERVERS: "kafka-broker:9092"
  #     KAFKA_TOPIC: "stock_data"          # change this

  #     # QuestDB connection settings (Postgres wire)
  #     QUESTDB_HOST: "questdb"
  #     QUESTDB_PORT: "8812"
  #     QUESTDB_DB: "qdb"
  #     QUESTDB_USER: "admin"
  #     QUESTDB_PASSWORD: "quest"
    # If you want to see aggregator logs easily in console:
    # logging:
    #   driver: "json-file"
    #   options:
    #     max-size: "10m"
    #     max-file: "3"

volumes:
  zookeeper_data:
  kafka_data:
  schema_registry_data:
  questdb-volume:
